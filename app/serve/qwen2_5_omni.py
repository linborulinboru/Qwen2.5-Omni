import io
import os
import ffmpeg
import numpy as np
import gradio as gr
import soundfile as sf
import datetime
import time
import threading
import logging
import gc
import torch
import shutil
from fastapi import FastAPI, UploadFile, File, BackgroundTasks
from argparse import ArgumentParser

import modelscope_studio.components.base as ms
import modelscope_studio.components.antd as antd
import gradio.processing_utils as processing_utils

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from gradio_client import utils as client_utils
from qwen_omni_utils import process_mm_info

# --- å…¨åŸŸè®Šæ•¸å’Œè¨­å®š ---
model = None
processor = None
args = None
last_activity_time = time.time()
model_lock = threading.Lock()
IDLE_TIMEOUT = 300  # é è¨­é–’ç½®300ç§’å¾Œå¸è¼‰æ¨¡å‹ï¼Œå¯ç”±å‘½ä»¤åˆ—åƒæ•¸è¦†è“‹

# --- æ—¥èªŒè¨­å®š ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s',
                    handlers=[logging.StreamHandler()])

# --- æç¤ºè©è¨­å®š ---
USER_PROMPT_BIBLE_TRANSCRIPTION = "è«‹å°‡éŸ³è¨Šå…§å®¹ç²¾ç¢ºè½‰éŒ„ç‚ºä¸­æ–‡æ–‡å­—ã€‚æ ¼å¼è¦æ±‚ï¼š1) æ¨™é»ç¬¦è™Ÿï¼šæ¯å¥è©±ä»¥å¥è™Ÿ(ã€‚)ã€å•è™Ÿ(?)æˆ–é©šå˜†è™Ÿ(!)çµå°¾,èªæ„åœé “è™•åŠ å…¥é€—è™Ÿ(,)ã€é “è™Ÿ(ã€)æˆ–åˆ†è™Ÿ(;) 2) è–ç¶“å¼•ç”¨æ ¼å¼ï¼šä½¿ç”¨ã€Šæ›¸å·åç« :ç¯€ã€‹æ ¼å¼,ä¾‹å¦‚ã€Šç´„ç¿°ç¦éŸ³3:16ã€‹ç¥æ„›ä¸–äºº,ç”šè‡³å°‡ä»–çš„ç¨ç”Ÿå­è³œçµ¦ä»–å€‘,å«ä¸€åˆ‡ä¿¡ä»–çš„,ä¸è‡´æ»…äº¡,åå¾—æ°¸ç”Ÿã€‚è–ç¶“æ›¸å·åŒ…å«ï¼šèˆŠç´„(å‰µä¸–è¨˜ã€å‡ºåŸƒåŠè¨˜ã€åˆ©æœªè¨˜ã€æ°‘æ•¸è¨˜ã€ç”³å‘½è¨˜ã€ç´„æ›¸äºè¨˜ã€å£«å¸«è¨˜ã€è·¯å¾—è¨˜ã€æ’’æ¯è€³è¨˜ä¸Šã€æ’’æ¯è€³è¨˜ä¸‹ã€åˆ—ç‹ç´€ä¸Šã€åˆ—ç‹ç´€ä¸‹ã€æ­·ä»£å¿—ä¸Šã€æ­·ä»£å¿—ä¸‹ã€ä»¥æ–¯æ‹‰è¨˜ã€å°¼å¸Œç±³è¨˜ã€ä»¥æ–¯å¸–è¨˜ã€ç´„ä¼¯è¨˜ã€è©©ç¯‡ã€ç®´è¨€ã€å‚³é“æ›¸ã€é›…æ­Œã€ä»¥è³½äºæ›¸ã€è€¶åˆ©ç±³æ›¸ã€è€¶åˆ©ç±³å“€æ­Œã€ä»¥è¥¿çµæ›¸ã€ä½†ä»¥ç†æ›¸ã€ä½•è¥¿é˜¿æ›¸ã€ç´„ç¥æ›¸ã€é˜¿æ‘©å¸æ›¸ã€ä¿„å·´åº•äºæ›¸ã€ç´„æ‹¿æ›¸ã€å½Œè¿¦æ›¸ã€é‚£é´»æ›¸ã€å“ˆå·´è°·æ›¸ã€è¥¿ç•ªé›…æ›¸ã€å“ˆè©²æ›¸ã€æ’’è¿¦åˆ©äºæ›¸ã€ç‘ªæ‹‰åŸºæ›¸)ã€æ–°ç´„(é¦¬å¤ªç¦éŸ³ã€é¦¬å¯ç¦éŸ³ã€è·¯åŠ ç¦éŸ³ã€ç´„ç¿°ç¦éŸ³ã€ä½¿å¾’ï¨ˆå‚³ã€ç¾…é¦¬æ›¸ã€å“¥æ—å¤šå‰æ›¸ã€å“¥æ—å¤šå¾Œæ›¸ã€åŠ ï¤¥å¤ªæ›¸ã€ä»¥å¼—æ‰€æ›¸ã€è…“ï§·æ¯”æ›¸ã€æ­Œï¤è¥¿æ›¸ã€å¸–æ’’ï¤å°¼è¿¦å‰æ›¸ã€å¸–æ’’ï¤å°¼è¿¦å¾Œæ›¸ã€ææ‘©å¤ªå‰æ›¸ã€ææ‘©å¤ªå¾Œæ›¸ã€æå¤šæ›¸ã€è…“ï§é–€æ›¸ã€å¸Œä¼¯ï¤­æ›¸ã€é›…å„æ›¸ã€å½¼å¾—å‰æ›¸ã€å½¼å¾—å¾Œæ›¸ã€ç´„ç¿°ä¸€æ›¸ã€ç´„ç¿°äºŒæ›¸ã€ç´„ç¿°ä¸‰æ›¸ã€çŒ¶å¤§æ›¸ã€å•Ÿç¤ºï¤¿) 3) ç›´æ¥è¼¸å‡ºè½‰éŒ„æ–‡å­—,ä¸åŒ…å«ä»»ä½•è§£é‡‹ã€è©•è«–ã€æ¨™è¨˜æˆ–å…ƒè³‡æ–™ã€‚"

# --- ç›®éŒ„å»ºç«‹ ---
os.makedirs('inputs', exist_ok=True)
os.makedirs('outputs', exist_ok=True)
os.makedirs('temp', exist_ok=True)


# --- æ¨¡å‹ç®¡ç†åŠŸèƒ½ ---
def _load_model_processor_internal():
    """å…§éƒ¨å‡½æ•¸ï¼Œç”¨æ–¼è¼‰å…¥æ¨¡å‹å’Œè™•ç†å™¨ã€‚"""
    global model, processor, args
    logging.info("é–‹å§‹è¼‰å…¥æ¨¡å‹...")
    if args.cpu_only:
        device_map = 'cpu'
    else:
        device_map = 'cuda'

    model_kwargs = {
        "torch_dtype": "auto",
        "device_map": device_map
    }
    if args.flash_attn2:
        model_kwargs["attn_implementation"] = "flash_attention_2"

    model = Qwen2_5OmniForConditionalGeneration.from_pretrained(args.checkpoint_path, **model_kwargs)
    processor = Qwen2_5OmniProcessor.from_pretrained(args.checkpoint_path)
    logging.info("æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
    return model, processor

def unload_model():
    """å¸è¼‰æ¨¡å‹ä»¥é‡‹æ”¾è¨˜æ†¶é«”ã€‚"""
    global model, processor, model_lock, IDLE_TIMEOUT
    with model_lock:
        if model is not None:
            logging.info(f"åµæ¸¬åˆ°é–’ç½®è¶…é {IDLE_TIMEOUT} ç§’ï¼Œé–‹å§‹å¸è¼‰æ¨¡å‹...")
            del model
            del processor
            model = None
            processor = None
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            logging.info("æ¨¡å‹å·²å¸è¼‰ï¼Œè¨˜æ†¶é«”å·²é‡‹æ”¾ã€‚")

def reset_idle_timer(new_thread=True):
    """é‡ç½®é–’ç½®è¨ˆæ™‚å™¨ã€‚"""
    global last_activity_time, IDLE_TIMEOUT
    last_activity_time = time.time()
    if new_thread:
        timer = threading.Timer(IDLE_TIMEOUT, unload_model)
        timer.daemon = True
        timer.start()

def get_model_and_processor():
    """ç²å–æ¨¡å‹å’Œè™•ç†å™¨ï¼Œå¦‚æœæœªè¼‰å…¥å‰‡è‡ªå‹•è¼‰å…¥ã€‚"""
    global model, processor, model_lock
    with model_lock:
        if model is None or processor is None:
            _load_model_processor_internal()
        reset_idle_timer()
        return model, processor

# --- æ ¸å¿ƒè½‰éŒ„é‚è¼¯ (é‡æ§‹å¾Œ) ---
def process_long_audio_yield_transcription(input_path: str):
    """
    ç”¢ç”Ÿå™¨å‡½å¼ï¼šåˆ†æ®µè™•ç†é•·éŸ³è¨Š/å½±ç‰‡æª”æ¡ˆï¼Œä¸¦é€æ®µç”¢ç”Ÿ(yield)è½‰éŒ„å¾Œçš„æ–‡å­—ã€‚
    """
    global args
    base_filename = os.path.splitext(os.path.basename(input_path))[0]

    try:
        probe = ffmpeg.probe(input_path)
        duration = float(probe['format']['duration'])
        logging.info(f"æª”æ¡ˆæ™‚é•·: {duration:.2f} ç§’")
    except ffmpeg.Error as e:
        logging.error(f"ç„¡æ³•è®€å–æª”æ¡ˆè³‡è¨Š {input_path}: {e.stderr.decode('utf8')}")
        return

    start_time = 0
    segment_index = 0
    segment_duration_val = args.segment_duration
    while start_time < duration:
        segment_duration = min(segment_duration_val, duration - start_time)
        if segment_duration < 1:  # å¿½ç•¥å¤ªçŸ­çš„å‰©é¤˜ç‰‡æ®µ
            break

        temp_wav_path = os.path.join('temp', f"{base_filename}_seg{segment_index}.wav")
        logging.info(f"æ­£åœ¨æå–ç¬¬ {segment_index + 1} æ®µ ({start_time:.2f}s - {start_time + segment_duration:.2f}s)...")

        try:
            (
                ffmpeg.input(input_path, ss=start_time, t=segment_duration)
                .output(temp_wav_path, ac=1, ar='16000', f='wav')  # å–®é€šé“, 16kHz WAV
                .run(quiet=True, overwrite_output=True)
            )
        except ffmpeg.Error as e:
            logging.error(f"FFmpeg æå–å¤±æ•—: {e.stderr.decode('utf8')}")
            start_time += segment_duration_val
            segment_index += 1
            continue

        logging.info(f"WAV æª”æ¡ˆå·²ä¿å­˜è‡³: {temp_wav_path}ï¼Œæº–å‚™é€²è¡Œè½‰éŒ„...")
        transcribed_text = ""
        try:
            current_model, current_processor = get_model_and_processor()
            messages = [{"role": "user", "content": [{"type": "text", "text": USER_PROMPT_BIBLE_TRANSCRIPTION}, {"type": "audio", "audio": temp_wav_path}]}]
            text = current_processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
            
            # --- BUG FIX START ---
            audios, _, _ = process_mm_info(messages, use_audio_in_video=True)
            # --- BUG FIX END ---
            
            inputs = current_processor(text=text, audio=audios, return_tensors="pt").to(current_model.device)

            gen_kwargs = {
                "max_new_tokens": args.max_new_tokens,
                "temperature": args.temperature,
                "repetition_penalty": args.repetition_penalty,
                "do_sample": True if args.temperature > 0 else False
            }
            generated_ids = current_model.generate(**inputs, **gen_kwargs)
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]
            response = current_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
            transcribed_text = response.strip()
            logging.info(f"ç¬¬ {segment_index + 1} æ®µè½‰éŒ„çµæœ: {transcribed_text}")
        
        except Exception as e:
            logging.error(f"è½‰éŒ„éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}", exc_info=True)
            transcribed_text = f"[ç¬¬ {segment_index + 1} æ®µè½‰éŒ„å¤±æ•—]"
        finally:
            if os.path.exists(temp_wav_path):
                os.remove(temp_wav_path)
                logging.info(f"å·²åˆªé™¤è‡¨æ™‚æª”æ¡ˆ: {temp_wav_path}")
        
        yield transcribed_text

        start_time += segment_duration_val
        segment_index += 1

def transcribe_file_task(input_path: str, output_filename: str):
    """Curl ä»»å‹™çš„èƒŒæ™¯è™•ç†å‡½å¼ï¼Œç¾åœ¨èª¿ç”¨æ ¸å¿ƒç”¢ç”Ÿå™¨ã€‚"""
    logging.info(f"é–‹å§‹èƒŒæ™¯è½‰éŒ„ä»»å‹™: {input_path}")
    output_txt_path = os.path.join('outputs', output_filename)
    with open(output_txt_path, 'a', encoding='utf-8') as f:
        for segment_text in process_long_audio_yield_transcription(input_path):
            f.write(segment_text + '\n')
    logging.info(f"æª”æ¡ˆ {input_path} è™•ç†å®Œæˆã€‚çµæœä¿å­˜åœ¨ {output_txt_path}")

# --- FastAPI æ‡‰ç”¨å’Œç«¯é» ---
app = FastAPI()

@app.get("/health")
async def health_check():
    """æä¾›ä¸€å€‹ç°¡å–®çš„å¥åº·æª¢æŸ¥ç«¯é»ã€‚"""
    return {"status": "ok"}

@app.post("/transcribe/")
async def create_transcription_job(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    """æ¥æ”¶éŸ³é »/è¦–é »æª”æ¡ˆï¼Œä¿å­˜ä¸¦å•Ÿå‹•èƒŒæ™¯è½‰éŒ„ä»»å‹™ã€‚"""
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    file_extension = os.path.splitext(file.filename)[1]
    input_filename = f"{timestamp}{file_extension}"
    input_filepath = os.path.join('inputs', input_filename)
    with open(input_filepath, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    output_txt_filename = f"{timestamp}.txt"
    logging.info(f"æ”¶åˆ°æª”æ¡ˆ: {file.filename}ï¼Œå·²ä¿å­˜è‡³ {input_filepath}ã€‚")
    background_tasks.add_task(transcribe_file_task, input_filepath, output_txt_filename)
    return {"message": "æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œå·²é–‹å§‹èƒŒæ™¯è½‰éŒ„è™•ç†ã€‚", "input_file": input_filepath, "output_file": os.path.join('outputs', output_txt_filename)}

# --- Gradio æ‡‰ç”¨é‚è¼¯ (å·²ä¿®æ”¹ä»¥æ”¯æ´æ¨¡å‹å¸è¼‰) ---
def _launch_demo(args_param):
    global args
    args = args_param
    
    if args.audio_only:
        default_prompt_for_ui = USER_PROMPT_BIBLE_TRANSCRIPTION
        is_interactive_prompt = False
        logging.info("åœ¨ audio-only æ¨¡å¼ä¸‹é‹è¡Œï¼Œä½¿ç”¨é è¨­çš„ ASR æç¤ºè©ã€‚")
    else:
        default_prompt_for_ui = 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'
        is_interactive_prompt = True

    language = args.ui_language
    
    def process_gradio_audio(audio_path, history, system_prompt):
        """è™•ç† Gradio ä¸Šå‚³çš„éŸ³è¨Šï¼Œä¸¦é€æ®µæ›´æ–° UIã€‚"""
        history.append({"role": "user", "content": (audio_path,)})
        history.append({"role": "assistant", "content": ""})
        yield history

        full_transcription = ""
        for segment_text in process_long_audio_yield_transcription(audio_path):
            full_transcription += segment_text + " "
            history[-1]["content"] = full_transcription.strip()
            yield history

    def predict_multimodal(history, system_prompt, voice_choice):
        """è™•ç†å¤šæ¨¡æ…‹è¼¸å…¥ï¼ˆæ–‡å­—ã€åœ–ç‰‡ã€çŸ­éŸ³è¨Š/å½±ç‰‡ï¼‰ï¼Œä¸é€²è¡Œåˆ†æ®µã€‚"""
        formatted_history = format_history(history, system_prompt)
        history.append({"role": "assistant", "content": ""})

        for chunk in predict(formatted_history, voice_choice):
            if chunk["type"] == "text":
                history[-1]["content"] = chunk["data"]
                yield history
            if chunk["type"] == "audio":
                history.append({"role": "assistant", "content": gr.Audio(chunk["data"])})
                yield history

    def format_history(history, system_prompt):
        messages = [{"role": "system", "content": [{"type": "text", "text": system_prompt}]}]
        for item in history:
            content = item.get("content")
            role = item.get("role")
            if not content or not role: continue
            if isinstance(content, str):
                messages.append({"role": role, "content": content})
            elif role == "user" and (isinstance(content, list) or isinstance(content, tuple)):
                file_path = content[0]
                mime_type = client_utils.get_mimetype(file_path)
                content_item = None
                if mime_type.startswith("image"): content_item = {"type": "image", "image": file_path}
                elif mime_type.startswith("video"): content_item = {"type": "video", "video": file_path}
                elif mime_type.startswith("audio"): content_item = {"type": "audio", "audio": file_path}
                if content_item: messages.append({"role": role, "content": [content_item]})
        return messages

    def predict(messages, voice='Chelsie'):
        current_model, current_processor = get_model_and_processor()
        text = current_processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(messages, use_audio_in_video=True)
        inputs = current_processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", padding=True, use_audio_in_video=True)
        inputs = inputs.to(current_model.device).to(current_model.dtype)
        gen_kwargs = {"max_new_tokens": args.max_new_tokens, "temperature": args.temperature, "repetition_penalty": args.repetition_penalty, "do_sample": True if args.temperature > 0 else False}
        
        if args.audio_only:
            text_ids = current_model.generate(**inputs, **gen_kwargs)
            audio = None
        else:
            text_ids, audio = current_model.generate(**inputs, speaker=voice, use_audio_in_video=True, **gen_kwargs)
        
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, text_ids)]
        response = current_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)

        response = response[0].split("\n")[-1] if response else ""
        yield {"type": "text", "data": response}
        
        if audio is not None:
            audio_np = np.array(audio * 32767).astype(np.int16)
            wav_io = io.BytesIO()
            sf.write(wav_io, audio_np, samplerate=24000, format="WAV")
            wav_io.seek(0)
            wav_bytes = wav_io.getvalue()
            audio_path = processing_utils.save_bytes_to_cache(wav_bytes, "audio.wav", cache_dir=demo.GRADIO_CACHE)
            yield {"type": "audio", "data": audio_path}

    def chat_predict_router(text, audio, image, video, history, system_prompt, voice_choice):
        """è·¯ç”±å‡½å¼ï¼Œæ ¹æ“šè¼¸å…¥é¡å‹æ±ºå®šè™•ç†æµç¨‹ã€‚"""
        if audio:
            for updated_history in process_gradio_audio(audio, history, system_prompt):
                yield gr.update(value=None), gr.update(value=None), gr.update(value=None), gr.update(value=None), updated_history
            return

        if text: history.append({"role": "user", "content": text})
        if image: history.append({"role": "user", "content": (image,)})
        if video: history.append({"role": "user", "content": (video,)})
        
        yield gr.update(value=None), gr.update(value=None), gr.update(value=None), gr.update(value=None), history

        for updated_history in predict_multimodal(history, system_prompt, voice_choice):
            yield gr.skip(), gr.skip(), gr.skip(), gr.skip(), updated_history


    with gr.Blocks() as demo:
        with gr.Sidebar(open=False):
            system_prompt_textbox = gr.Textbox(label="System Prompt", value=default_prompt_for_ui, interactive=is_interactive_prompt)
        with antd.Flex(gap="small", justify="center", align="center"):
            with antd.Flex(vertical=True, gap="small", align="center"):
                antd.Typography.Title("Qwen2.5-Omni Demo", level=1, elem_style=dict(margin=0, fontSize=28))
                with antd.Flex(vertical=True, gap="small"):
                    antd.Typography.Text("ğŸ¯ ä½¿ç”¨èªªæ˜ï¼š", strong=True)
                    antd.Typography.Text("1ï¸âƒ£ é»æ“ŠéŸ³è¨ŠéŒ„è£½æŒ‰éˆ•ï¼Œæˆ–ä¸Šå‚³æª”æ¡ˆ")
                    antd.Typography.Text("2ï¸âƒ£ è¼¸å…¥éŸ³è¨Šé€²è¡Œè½‰éŒ„ï¼Œæˆ–èˆ‡æ¨¡å‹é€²è¡Œå¤šæ¨¡æ…‹å°è©±")
                    antd.Typography.Text("3ï¸âƒ£ é»æ“Šæäº¤ä¸¦ç­‰å¾…æ¨¡å‹çš„å›ç­”")
        voice_choice = gr.Dropdown(label="Voice Choice", choices=['Chelsie', 'Ethan'], value='Chelsie', visible=not args.audio_only)
        
        with gr.Tabs():
            with gr.Tab("Online"):
                with gr.Row():
                    with gr.Column(scale=1):
                        microphone = gr.Audio(sources=['microphone'], type="filepath")
                        webcam = gr.Video(sources=['webcam'], height=400, include_audio=True, visible=not args.audio_only)
                        submit_btn = gr.Button("æäº¤", variant="primary")
                        stop_btn = gr.Button("åœæ­¢", visible=False)
                        clear_btn = gr.Button("æ¸…é™¤æ­·å²")
                    with gr.Column(scale=2):
                        media_chatbot = gr.Chatbot(height=650, type="messages")
                def clear_history(): return [], gr.update(value=None), gr.update(value=None)
                def media_predict(audio, video, history, system_prompt, voice_choice): # çŸ­éŸ³è¨Šè™•ç†
                    if audio: history.append({"role": "user", "content": (audio,)})
                    if video: 
                        video_path = video.replace('.webm', '.mp4')
                        ffmpeg.input(video).output(video_path, y='-y').run(quiet=True, overwrite_output=True)
                        history.append({"role": "user", "content": (video_path,)})
                    
                    yield gr.update(value=None), gr.update(value=None), history
                    
                    for updated_history in predict_multimodal(history, system_prompt, voice_choice):
                        yield gr.update(value=None), gr.update(value=None), updated_history

                submit_event = submit_btn.click(fn=media_predict, inputs=[microphone, webcam, media_chatbot, system_prompt_textbox, voice_choice], outputs=[microphone, webcam, media_chatbot])
                stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[submit_event], queue=False)
                clear_btn.click(fn=clear_history, inputs=None, outputs=[media_chatbot, microphone, webcam])


            with gr.Tab("Offline"):
                chatbot = gr.Chatbot(type="messages", height=650)
                with gr.Row(equal_height=True):
                    audio_input = gr.Audio(sources=["upload"], type="filepath", label="ä¸Šå‚³éŸ³è¨Š (é•·/çŸ­)", elem_classes="media-upload", scale=1)
                    image_input = gr.Image(sources=["upload"], type="filepath", label="ä¸Šå‚³åœ–ç‰‡", elem_classes="media-upload", scale=1, visible=not args.audio_only)
                    video_input = gr.Video(sources=["upload"], label="ä¸Šå‚³å½±ç‰‡ (çŸ­)", elem_classes="media-upload", scale=1, visible=not args.audio_only)
                text_input = gr.Textbox(show_label=False, placeholder="è¼¸å…¥æ–‡å­—...")
                with gr.Row():
                    submit_btn_offline = gr.Button("æäº¤", variant="primary", size="lg")
                    stop_btn_offline = gr.Button("åœæ­¢", visible=False, size="lg")
                    clear_btn_offline = gr.Button("æ¸…é™¤æ­·å²", size="lg")
                def clear_chat_history(): return [], gr.update(value=None), gr.update(value=None), gr.update(value=None), gr.update(value=None)
                
                submit_event_offline = gr.on(
                    triggers=[submit_btn_offline.click, text_input.submit], 
                    fn=chat_predict_router, 
                    inputs=[text_input, audio_input, image_input, video_input, chatbot, system_prompt_textbox, voice_choice], 
                    outputs=[text_input, audio_input, image_input, video_input, chatbot]
                )
                stop_btn_offline.click(fn=None, inputs=None, outputs=None, cancels=[submit_event_offline], queue=False)
                clear_btn_offline.click(fn=clear_chat_history, inputs=None, outputs=[chatbot, text_input, audio_input, image_input, video_input])

                gr.HTML("""
                    <style>
                        .media-upload { margin: 10px; min-height: 160px; }
                        .media-upload > .wrap { border: 2px dashed #ccc; border-radius: 8px; padding: 10px; height: 100%; }
                        .media-upload:hover > .wrap { border-color: #666; }
                        .media-upload { flex: 1; min-width: 0; }
                    </style>
                """)
    return demo


def _get_args():
    parser = ArgumentParser()
    parser.add_argument('-c', '--checkpoint-path', type=str, default="Qwen/Qwen2.5-Omni-7B", help='Checkpoint name or path, default to %(default)r')
    parser.add_argument('--cpu-only', action='store_true', help='Run demo with CPU only')
    parser.add_argument('--flash-attn2', action='store_true', default=False, help='Enable flash_attention_2 when loading the model.')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='Demo server name.')
    parser.add_argument('--port', type=int, default=7860, help='Demo server port.')
    parser.add_argument('--audio-only', action='store_true', help='Run in audio-only mode, hiding video components in the UI.')
    parser.add_argument('--segment-duration', type=int, default=60, help='Duration of each audio segment for transcription in seconds.')
    parser.add_argument('--max-new-tokens', type=int, default=1536, help='Maximum new tokens to generate.')
    parser.add_argument('--temperature', type=float, default=0.1, help='Temperature for sampling.')
    parser.add_argument('--repetition-penalty', type=float, default=1.1, help='Repetition penalty.')
    parser.add_argument('--idle-timeout', type=int, default=300, help='Idle timeout in seconds before unloading the model.')
    parser.add_argument('--ui-language', type=str, choices=['en', 'zh'], default='zh', help='Display language for the UI.')
    return parser.parse_args()


if __name__ == "__main__":
    cli_args = _get_args()
    IDLE_TIMEOUT = cli_args.idle_timeout
    gradio_app = _launch_demo(cli_args)
    app = gr.mount_gradio_app(app, gradio_app, path="/")
    reset_idle_timer()
    logging.info(f"å•Ÿå‹•ä¼ºæœå™¨æ–¼ http://{cli_args.host}:{cli_args.port}")
    logging.info(f"Gradio UI ä»‹é¢ä½æ–¼ http://{cli_args.host}:{cli_args.port}/")
    logging.info(f"Curl API ç«¯é»ä½æ–¼ http://{cli_args.host}:{cli_args.port}/transcribe/")
    import uvicorn
    uvicorn.run(app, host=cli_args.host, port=cli_args.port)

