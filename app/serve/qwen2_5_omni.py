import io
import os
import ffmpeg
import numpy as np
import gradio as gr
import soundfile as sf
import datetime
import time
import threading
import logging
import gc
import torch
import shutil
import uuid
from fastapi import FastAPI, UploadFile, File, BackgroundTasks
from argparse import ArgumentParser
from opencc import OpenCC

import modelscope_studio.components.base as ms
import modelscope_studio.components.antd as antd
import gradio.processing_utils as processing_utils

from transformers import Qwen2_5OmniForConditionalGeneration, Qwen2_5OmniProcessor
from gradio_client import utils as client_utils
from qwen_omni_utils import process_mm_info

# --- å…¨åŸŸè®Šæ•¸å’Œè¨­å®š ---
model = None
processor = None
opencc_converter = None  # OpenCC converter for Traditional Chinese
args = None
last_activity_time = time.time()
model_lock = threading.Lock()
processing_lock = threading.Lock()  # ç”¨æ–¼åºåˆ—åŒ–è«‹æ±‚è™•ç†
IDLE_TIMEOUT = 300  # é è¨­é–’ç½®300ç§’å¾Œå¸è¼‰æ¨¡å‹ï¼Œå¯ç”±å‘½ä»¤åˆ—åƒæ•¸è¦†è“‹

# é–’ç½®è¿½è¹¤
is_processing = False  # æ˜¯å¦æ­£åœ¨è™•ç†ä¸­


# --- æç¤ºè©è¨­å®š ---
USER_PROMPT_BIBLE_TRANSCRIPTION = "è«‹å°‡éŸ³è¨Šå…§å®¹ç²¾ç¢ºè½‰éŒ„ç‚ºä¸­æ–‡æ–‡å­—ã€‚æ ¼å¼è¦æ±‚ï¼š1) æ¨™é»ç¬¦è™Ÿï¼šæ¯å¥è©±ä»¥å¥è™Ÿ(ã€‚)ã€å•è™Ÿ(?)æˆ–é©šå˜†è™Ÿ(!)çµå°¾,èªæ„åœé “è™•åŠ å…¥é€—è™Ÿ(,)ã€é “è™Ÿ(ã€)æˆ–åˆ†è™Ÿ(;) 2) è–ç¶“å¼•ç”¨æ ¼å¼ï¼šä½¿ç”¨ã€Šæ›¸å·åç« :ç¯€ã€‹æ ¼å¼,ä¾‹å¦‚ã€Šç´„ç¿°ç¦éŸ³3:16ã€‹ç¥æ„›ä¸–äºº,ç”šè‡³å°‡ä»–çš„ç¨ç”Ÿå­è³œçµ¦ä»–å€‘,å«ä¸€åˆ‡ä¿¡ä»–çš„,ä¸è‡´æ»…äº¡,åå¾—æ°¸ç”Ÿã€‚è–ç¶“æ›¸å·åŒ…å«ï¼šèˆŠç´„(å‰µä¸–è¨˜ã€å‡ºåŸƒåŠè¨˜ã€åˆ©æœªè¨˜ã€æ°‘æ•¸è¨˜ã€ç”³å‘½è¨˜ã€ç´„æ›¸äºè¨˜ã€å£«å¸«è¨˜ã€è·¯å¾—è¨˜ã€æ’’æ¯è€³è¨˜ä¸Šã€æ’’æ¯è€³è¨˜ä¸‹ã€åˆ—ç‹ç´€ä¸Šã€åˆ—ç‹ç´€ä¸‹ã€æ­·ä»£å¿—ä¸Šã€æ­·ä»£å¿—ä¸‹ã€ä»¥æ–¯æ‹‰è¨˜ã€å°¼å¸Œç±³è¨˜ã€ä»¥æ–¯å¸–è¨˜ã€ç´„ä¼¯è¨˜ã€è©©ç¯‡ã€ç®´è¨€ã€å‚³é“æ›¸ã€é›…æ­Œã€ä»¥è³½äºæ›¸ã€è€¶åˆ©ç±³æ›¸ã€è€¶åˆ©ç±³å“€æ­Œã€ä»¥è¥¿çµæ›¸ã€ä½†ä»¥ç†æ›¸ã€ä½•è¥¿é˜¿æ›¸ã€ç´„ç¥æ›¸ã€é˜¿æ‘©å¸æ›¸ã€ä¿„å·´åº•äºæ›¸ã€ç´„æ‹¿æ›¸ã€å½Œè¿¦æ›¸ã€é‚£é´»æ›¸ã€å“ˆå·´è°·æ›¸ã€è¥¿ç•ªé›…æ›¸ã€å“ˆè©²æ›¸ã€æ’’è¿¦åˆ©äºæ›¸ã€ç‘ªæ‹‰åŸºæ›¸)ã€æ–°ç´„(é¦¬å¤ªç¦éŸ³ã€é¦¬å¯ç¦éŸ³ã€è·¯åŠ ç¦éŸ³ã€ç´„ç¿°ç¦éŸ³ã€ä½¿å¾’ï¨ˆå‚³ã€ç¾…é¦¬æ›¸ã€å“¥æ—å¤šå‰æ›¸ã€å“¥æ—å¤šå¾Œæ›¸ã€åŠ ï¤¥å¤ªæ›¸ã€ä»¥å¼—æ‰€æ›¸ã€è…“ï§·æ¯”æ›¸ã€æ­Œï¤è¥¿æ›¸ã€å¸–æ’’ï¤å°¼è¿¦å‰æ›¸ã€å¸–æ’’ï¤å°¼è¿¦å¾Œæ›¸ã€ææ‘©å¤ªå‰æ›¸ã€ææ‘©å¤ªå¾Œæ›¸ã€æå¤šæ›¸ã€è…“ï§é–€æ›¸ã€å¸Œä¼¯ï¤­æ›¸ã€é›…å„æ›¸ã€å½¼å¾—å‰æ›¸ã€å½¼å¾—å¾Œæ›¸ã€ç´„ç¿°ä¸€æ›¸ã€ç´„ç¿°äºŒæ›¸ã€ç´„ç¿°ä¸‰æ›¸ã€çŒ¶å¤§æ›¸ã€å•Ÿç¤ºï¤¿) 3) ç›´æ¥è¼¸å‡ºè½‰éŒ„æ–‡å­—,ä¸åŒ…å«ä»»ä½•è§£é‡‹ã€è©•è«–ã€æ¨™è¨˜æˆ–å…ƒè³‡æ–™ã€‚"

# --- ç›®éŒ„å»ºç«‹ ---
os.makedirs('inputs', exist_ok=True)
os.makedirs('outputs', exist_ok=True)
os.makedirs('temp', exist_ok=True)


# --- æ¨¡å‹ç®¡ç†åŠŸèƒ½ ---
def _load_model_processor_internal():
    """å…§éƒ¨å‡½æ•¸ï¼Œç”¨æ–¼è¼‰å…¥æ¨¡å‹å’Œè™•ç†å™¨ã€‚"""
    global model, processor, args, opencc_converter
    print("[INFO] é–‹å§‹è¼‰å…¥æ¨¡å‹...")
    if args.cpu_only:
        device_map = 'cpu'
    else:
        device_map = 'cuda'

    model_kwargs = {
        "torch_dtype": "auto",
        "device_map": device_map
    }
    if args.flash_attn2:
        model_kwargs["attn_implementation"] = "flash_attention_2"

    model = Qwen2_5OmniForConditionalGeneration.from_pretrained(args.checkpoint_path, **model_kwargs)
    processor = Qwen2_5OmniProcessor.from_pretrained(args.checkpoint_path)
    
    # åˆå§‹åŒ– OpenCC è½‰æ›å™¨
    try:
        opencc_converter = OpenCC('s2tw')  # ç°¡é«”åˆ°å°ç£ç¹é«”
        print("[INFO] OpenCC è½‰æ›å™¨åˆå§‹åŒ–å®Œæˆ (ç°¡é«”åˆ°å°ç£ç¹é«”)")
    except Exception as e:
        print(f"[WARNING] OpenCC è½‰æ›å™¨åˆå§‹åŒ–å¤±æ•—: {e}")
        opencc_converter = None

    print("[INFO] æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")
    return model, processor

def unload_model():
    """å¸è¼‰æ¨¡å‹ä¸¦æ¸…é™¤ CUDA ç·©å­˜ - å…§éƒ¨ä½¿ç”¨ï¼ˆå‡è¨­é–å·²è¢«ç²å–ï¼‰"""
    global model, processor

    if model is None:
        return

    print("[INFO] ç”±æ–¼é–’ç½®è¶…æ™‚ï¼Œæ­£åœ¨å¸è¼‰æ¨¡å‹...")

    try:
        # å¸è¼‰å‰å°‡æ¨¡å‹çµ„ä»¶ç§»è‡³ CPU ä»¥é‡‹æ”¾ GPU è¨˜æ†¶é«”
        if torch.cuda.is_available():
            print("[INFO] æ­£åœ¨å°‡æ¨¡å‹çµ„ä»¶ç§»è‡³ CPU...")
            try:
                # ç§»å‹•ä¸»è¦æ¨¡å‹çµ„ä»¶è‡³ CPU
                if hasattr(model, 'model'):
                    if hasattr(model.model, 'thinker'):
                        model.model.thinker = model.model.thinker.cpu()
                    # å¦‚æœæ˜¯ Qwen2_5OmniForConditionalGeneration é¡å‹
                    elif hasattr(model.model, 'language_model'):
                        model.model.language_model = model.model.language_model.cpu()
                elif hasattr(model, 'cpu'):
                    model.cpu()
            except Exception as e:
                print(f"[WARNING] å°‡æ¨¡å‹ç§»è‡³ CPU æ™‚å‡ºéŒ¯: {e}")

        # åˆªé™¤æ¨¡å‹å’Œè™•ç†å™¨
        del model
        del processor
        model = None
        processor = None

        # æ¸…é™¤ CUDA ç·©å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()

            # åˆ—å°è¨˜æ†¶é«”ç‹€æ…‹
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            print(f"[INFO] å¸è¼‰å¾Œ GPU è¨˜æ†¶é«”: {allocated:.2f} GB å·²åˆ†é…, {reserved:.2f} GB å·²ä¿ç•™")

        print("[INFO] æ¨¡å‹å·²å¸è¼‰ä¸¦æ¸…é™¤ CUDA ç·©å­˜")

    except Exception as e:
        print(f"[ERROR] å¸è¼‰æ¨¡å‹æ™‚å‡ºéŒ¯: {e}")
        import traceback
        traceback.print_exc()

def update_activity():
    """æ›´æ–°æœ€å¾Œæ´»å‹•æ™‚é–“æˆ³"""
    global last_activity_time
    last_activity_time = time.time()

def idle_monitor():
    """ç›£æ§é–’ç½®æ™‚é–“ï¼Œå¦‚æœé–’ç½®å¤ªä¹…å‰‡å¸è¼‰æ¨¡å‹"""
    global model, is_processing, last_activity_time
    idle_check_interval = 30  # æ¯30ç§’æª¢æŸ¥ä¸€æ¬¡
    idle_timeout = IDLE_TIMEOUT  # ä½¿ç”¨å‘½ä»¤è¡Œåƒæ•¸è¨­ç½®çš„è¶…æ™‚æ™‚é–“

    while True:
        time.sleep(idle_check_interval)

        # è·³éæ­£åœ¨è™•ç†çš„æƒ…æ³
        if is_processing:
            continue

        # è·³éæ²’æœ‰æ´»å‹•è¨˜éŒ„çš„æƒ…æ³
        if last_activity_time is None:
            continue

        # æª¢æŸ¥é–’ç½®æ™‚é–“
        idle_time = time.time() - last_activity_time

        if idle_time >= idle_timeout:
            # ç²å–é–ä¸¦å¸è¼‰æ¨¡å‹
            with model_lock:
                # ç²å–é–å¾Œå†æ¬¡æª¢æŸ¥æ¢ä»¶
                if model is not None and not is_processing:
                    print(f"[INFO] æ¨¡å‹é–’ç½® {idle_time:.0f} ç§’ï¼Œæ­£åœ¨å¸è¼‰...")
                    # èª¿ç”¨ unload_modelï¼ˆå®ƒæœ¬èº«ä¸ç²å–é–ï¼‰
                    unload_model()
                    last_activity_time = None

def get_model_and_processor():
    """ç²å–æ¨¡å‹å’Œè™•ç†å™¨ï¼Œå¦‚æœæœªè¼‰å…¥å‰‡è‡ªå‹•è¼‰å…¥ã€‚"""
    global model, processor, model_lock
    with model_lock:
        if model is None or processor is None:
            _load_model_processor_internal()
        update_activity()
        return model, processor

def _generate_request_id():
    """ç”¢ç”Ÿå”¯ä¸€çš„è«‹æ±‚ IDã€‚"""
    return f"{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}-{uuid.uuid4().hex[:6]}"

# --- æ ¸å¿ƒè½‰éŒ„é‚è¼¯ (é‡æ§‹å¾Œ) ---
def process_long_audio_yield_transcription(input_path: str, request_id: str):
    """
    ç”¢ç”Ÿå™¨å‡½å¼ï¼šåˆ†æ®µè™•ç†é•·éŸ³è¨Š/å½±ç‰‡æª”æ¡ˆï¼Œä¸¦é€æ®µç”¢ç”Ÿ(yield)è½‰éŒ„å¾Œçš„æ–‡å­—ã€‚
    """
    global args
    base_filename = os.path.splitext(os.path.basename(input_path))[0]

    try:
        probe = ffmpeg.probe(input_path)
        duration = float(probe['format']['duration'])
        print(f"[{request_id}] æª”æ¡ˆæ™‚é•·: {duration:.2f} ç§’")
    except ffmpeg.Error as e:
        import traceback
        print(f"[{request_id}] ç„¡æ³•è®€å–æª”æ¡ˆè³‡è¨Š {input_path}: {e.stderr.decode('utf8')}")
        traceback.print_exc()
        return

    start_time = 0
    segment_index = 0
    segment_duration_val = args.segment_duration
    total_segments = int(np.ceil(duration / segment_duration_val))

    while start_time < duration:
        segment_duration = min(segment_duration_val, duration - start_time)
        if segment_duration < 1:
            break

        temp_wav_path = os.path.join('temp', f"{base_filename}_seg{segment_index}.wav")
        print(f"[{request_id}] æ­£åœ¨æå–ç¬¬ {segment_index + 1}/{total_segments} æ®µ ({start_time:.2f}s - {start_time + segment_duration:.2f}s)...")

        try:
            (
                ffmpeg.input(input_path, ss=start_time, t=segment_duration)
                .output(temp_wav_path, ac=1, ar='16000', f='wav')
                .run(quiet=True, overwrite_output=True)
            )
        except ffmpeg.Error as e:
            import traceback
            print(f"[{request_id}] FFmpeg æå–å¤±æ•—: {e.stderr.decode('utf8')}")
            traceback.print_exc()
            start_time += segment_duration_val
            segment_index += 1
            continue
        
        transcribed_text = ""
        try:
            current_model, current_processor = get_model_and_processor()
            messages = [{"role": "user", "content": [{"type": "text", "text": USER_PROMPT_BIBLE_TRANSCRIPTION}, {"type": "audio", "audio": temp_wav_path}]}]
            text = current_processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
            audios, _, _ = process_mm_info(messages, use_audio_in_video=True)
            inputs = current_processor(text=text, audio=audios, return_tensors="pt", padding=True, use_audio_in_video=True).to(current_model.device)
            gen_kwargs = {
                "max_new_tokens": args.max_new_tokens,
                "temperature": args.temperature,
                "repetition_penalty": args.repetition_penalty,
                "do_sample": True if args.temperature > 0 else False,
                "use_audio_in_video": True
            }
            generated_ids = current_model.generate(**inputs, **gen_kwargs)
            generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]
            decoded_list = current_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
            transcribed_text = decoded_list[0] if decoded_list else ""

            # ä½¿ç”¨ OpenCC è½‰æ›ç‚ºå°ç£ç¹é«”ä¸­æ–‡
            if opencc_converter is not None:
                transcribed_text = opencc_converter.convert(transcribed_text)
                print(f"[{request_id}] å·²å°‡è½‰éŒ„çµæœè½‰æ›ç‚ºå°ç£ç¹é«”ä¸­æ–‡")

            # --- ç¯„æœ¬æ ¼å¼æ—¥èªŒè¼¸å‡º ---
            print(f"[{request_id}] ====== SEGMENT {segment_index+1}/{total_segments} RESULT (starts at {start_time/60:.1f} mins) ======")
            print(transcribed_text)
            print(f"[{request_id}] ====== END OF SEGMENT {segment_index+1} ({len(transcribed_text)} chars) ======")

        except Exception as e:
            import traceback
            print(f"[{request_id}] è½‰éŒ„éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            traceback.print_exc()
            transcribed_text = f"[ç¬¬ {segment_index + 1} æ®µè½‰éŒ„å¤±æ•—]"
        finally:
            if os.path.exists(temp_wav_path):
                os.remove(temp_wav_path)
        
        yield transcribed_text

        start_time += segment_duration_val
        segment_index += 1

def transcribe_file_task(input_path: str, output_filename: str, request_id: str):
    """Curl ä»»å‹™çš„èƒŒæ™¯è™•ç†å‡½å¼ã€‚"""
    global is_processing
    
    # æ¨™è¨˜ç‚ºæ­£åœ¨è™•ç†ï¼Œé˜²æ­¢æ¨¡å‹å¸è¼‰
    is_processing = True
    update_activity()
    
    try:
        print(f"[{request_id}] é–‹å§‹èƒŒæ™¯è½‰éŒ„ä»»å‹™: {input_path}")
        output_txt_path = os.path.join('outputs', output_filename)
        all_segments_text = []
        with open(output_txt_path, 'a', encoding='utf-8') as f:
            for segment_text in process_long_audio_yield_transcription(input_path, request_id):
                f.write(segment_text + '\n')
                all_segments_text.append(segment_text)

        merged_result = "\n".join(all_segments_text)
        
        # --- ç¯„æœ¬æ ¼å¼æ—¥èªŒè¼¸å‡º ---
        print(f"[{request_id}] ====== FINAL COMBINED TRANSCRIPTION ======")
        print(merged_result)
        print(f"[{request_id}] ====== END OF COMBINED TRANSCRIPTION ({len(merged_result)} chars) ======")
        
        print(f"[{request_id}] ====== SAVED TRANSCRIPTION to {output_txt_path} ======")
        print(merged_result)
        print(f"[{request_id}] ====== END OF SAVED TRANSCRIPTION ======")
        
        print(f"[{request_id}] æª”æ¡ˆ {input_path} è™•ç†å®Œæˆã€‚")
    finally:
        # æ¨™è¨˜ç‚ºè™•ç†å®Œæˆï¼Œå…è¨±æ¨¡å‹å¸è¼‰
        is_processing = False
        update_activity()

# --- FastAPI æ‡‰ç”¨å’Œç«¯é» ---
app = FastAPI()

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.post("/transcribe/")
async def create_transcription_job(background_tasks: BackgroundTasks, file: UploadFile = File(...)):
    request_id = _generate_request_id()
    timestamp = datetime.datetime.now().strftime("%Y%m%d%H%M%S")
    file_extension = os.path.splitext(file.filename)[1]
    input_filename = f"{timestamp}{file_extension}"
    input_filepath = os.path.join('inputs', input_filename)
    with open(input_filepath, "wb") as buffer:
        shutil.copyfileobj(file.file, buffer)
    output_txt_filename = f"{timestamp}.txt"
    print(f"[{request_id}] æ”¶åˆ°æª”æ¡ˆ: {file.filename}ï¼Œå·²ä¿å­˜è‡³ {input_filepath}ã€‚")
    
    # ä½¿ç”¨åºåˆ—åŒ–é–è™•ç†è«‹æ±‚
    def task_wrapper():
        with processing_lock:
            global is_processing
            is_processing = True
            update_activity()
            try:
                transcribe_file_task(input_filepath, output_txt_filename, request_id)
            finally:
                is_processing = False
                update_activity()
    
    background_tasks.add_task(task_wrapper)
    return {"message": "æª”æ¡ˆä¸Šå‚³æˆåŠŸï¼Œå·²é–‹å§‹èƒŒæ™¯è½‰éŒ„è™•ç†ã€‚", "request_id": request_id, "output_file": os.path.join('outputs', output_txt_filename)}

# --- Gradio æ‡‰ç”¨é‚è¼¯ ---
def _launch_demo(args_param):
    global args
    args = args_param
    
    if args.audio_only:
        default_prompt_for_ui = USER_PROMPT_BIBLE_TRANSCRIPTION
        is_interactive_prompt = False
        print("[INFO] åœ¨ audio-only æ¨¡å¼ä¸‹é‹è¡Œï¼Œä½¿ç”¨é è¨­çš„ ASR æç¤ºè©ã€‚")
    else:
        default_prompt_for_ui = 'You are Qwen, a virtual human...'
        is_interactive_prompt = True

    def process_gradio_audio(audio_path, history, system_prompt):
        global is_processing
        
        # æ¨™è¨˜ç‚ºæ­£åœ¨è™•ç†ï¼Œé˜²æ­¢æ¨¡å‹å¸è¼‰
        is_processing = True
        update_activity()
        
        try:
            request_id = _generate_request_id()
            history.append({"role": "user", "content": (audio_path,)})
            history.append({"role": "assistant", "content": ""})
            yield history

            full_transcription = ""
            for segment_text in process_long_audio_yield_transcription(audio_path, request_id):
                full_transcription += segment_text + " "
                history[-1]["content"] = full_transcription.strip()
                yield history
            
            print(f"[{request_id}] ====== FINAL GRADIO TRANSCRIPTION ======")
            print(full_transcription.strip())
            print(f"[{request_id}] ====== END OF GRADIO TRANSCRIPTION ({len(full_transcription.strip())} chars) ======")
        finally:
            # æ¨™è¨˜ç‚ºè™•ç†å®Œæˆï¼Œå…è¨±æ¨¡å‹å¸è¼‰
            is_processing = False
            update_activity()

    def predict_multimodal(history, system_prompt, voice_choice):
        global is_processing
        
        # æ¨™è¨˜ç‚ºæ­£åœ¨è™•ç†ï¼Œé˜²æ­¢æ¨¡å‹å¸è¼‰
        is_processing = True
        update_activity()
        
        try:
            request_id = _generate_request_id()
            formatted_history = format_history(history, system_prompt)
            history.append({"role": "assistant", "content": ""})

            for chunk in predict(formatted_history, voice_choice, request_id):
                if chunk["type"] == "text":
                    history[-1]["content"] = chunk["data"]
                    yield history
                if chunk["type"] == "audio":
                    history.append({"role": "assistant", "content": gr.Audio(chunk["data"])})
                    yield history
        finally:
            # æ¨™è¨˜ç‚ºè™•ç†å®Œæˆï¼Œå…è¨±æ¨¡å‹å¸è¼‰
            is_processing = False
            update_activity()

    def format_history(history, system_prompt):
        messages = [{"role": "system", "content": [{"type": "text", "text": system_prompt}]}]
        for item in history:
            content, role = item.get("content"), item.get("role")
            if not content or not role: continue
            if isinstance(content, str):
                messages.append({"role": role, "content": content})
            elif role == "user" and (isinstance(content, list) or isinstance(content, tuple)):
                file_path = content[0]
                mime_type = client_utils.get_mimetype(file_path)
                content_item = None
                if mime_type.startswith("image"): content_item = {"type": "image", "image": file_path}
                elif mime_type.startswith("video"): content_item = {"type": "video", "video": file_path}
                elif mime_type.startswith("audio"): content_item = {"type": "audio", "audio": file_path}
                if content_item: messages.append({"role": role, "content": [content_item]})
        return messages

    def predict(messages, voice, request_id):
        current_model, current_processor = get_model_and_processor()
        text = current_processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        audios, images, videos = process_mm_info(messages, use_audio_in_video=True)
        inputs = current_processor(text=text, audio=audios, images=images, videos=videos, return_tensors="pt", padding=True, use_audio_in_video=True).to(current_model.device)
        gen_kwargs = {"max_new_tokens": args.max_new_tokens, "temperature": args.temperature, "repetition_penalty": args.repetition_penalty, "do_sample": True if args.temperature > 0 else False, "use_audio_in_video": True}
        
        if args.audio_only or not audios:
             text_ids = current_model.generate(**inputs, **gen_kwargs)
             audio = None
        else:
             text_ids, audio = current_model.generate(**inputs, speaker=voice, **gen_kwargs)
        
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, text_ids)]
        decoded_list = current_processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)
        response = decoded_list[0] if decoded_list else ""
        
        # ä½¿ç”¨ OpenCC è½‰æ›ç‚ºå°ç£ç¹é«”ä¸­æ–‡
        if opencc_converter is not None and audios:  # åªåœ¨æœ‰éŸ³è¨Šè¼¸å…¥æ™‚é€²è¡Œè½‰æ›
            response = opencc_converter.convert(response)
            print(f"[{request_id}] å·²å°‡è½‰éŒ„çµæœè½‰æ›ç‚ºå°ç£ç¹é«”ä¸­æ–‡")
        
        # --- ç¯„æœ¬æ ¼å¼æ—¥èªŒè¼¸å‡º (for short audio/text) ---
        if audios: # åªåœ¨æœ‰éŸ³è¨Šè¼¸å…¥æ™‚æ‰“å°è½‰éŒ„æ—¥èªŒ
            print(f"[{request_id}] ====== TRANSCRIPTION RESULT ======")
            print(response)
            print(f"[{request_id}] ====== END OF TRANSCRIPTION ({len(response)} chars) ======")

        yield {"type": "text", "data": response}
        
        if audio is not None:
            audio_np = np.array(audio * 32767).astype(np.int16)
            wav_io, audio_path = io.BytesIO(), None
            sf.write(wav_io, audio_np, samplerate=24000, format="WAV")
            wav_io.seek(0)
            audio_path = processing_utils.save_bytes_to_cache(wav_io.getvalue(), "audio.wav", cache_dir=demo.GRADIO_CACHE)
            yield {"type": "audio", "data": audio_path}

    def chat_predict_router(text, audio, image, video, history, system_prompt, voice_choice):
        if audio:
            for updated_history in process_gradio_audio(audio, history, system_prompt):
                yield gr.update(value=None), gr.update(value=None), gr.update(value=None), gr.update(value=None), updated_history
            return

        if text: history.append({"role": "user", "content": text})
        if image: history.append({"role": "user", "content": (image,)})
        if video: history.append({"role": "user", "content": (video,)})
        
        yield gr.update(value=None), gr.update(value=None), gr.update(value=None), gr.update(value=None), history

        for updated_history in predict_multimodal(history, system_prompt, voice_choice):
            yield gr.skip(), gr.skip(), gr.skip(), gr.skip(), updated_history

    with gr.Blocks() as demo:
        with gr.Sidebar(open=False):
            system_prompt_textbox = gr.Textbox(label="System Prompt", value=default_prompt_for_ui, interactive=is_interactive_prompt)
        # ... UI layout code ... (ä¿æŒä¸è®Š)
        with antd.Flex(gap="small", justify="center", align="center"):
            with antd.Flex(vertical=True, gap="small", align="center"):
                antd.Typography.Title("Qwen2.5-Omni Demo", level=1, elem_style=dict(margin=0, fontSize=28))
                with antd.Flex(vertical=True, gap="small"):
                    antd.Typography.Text("ğŸ¯ ä½¿ç”¨èªªæ˜ï¼š", strong=True)
                    antd.Typography.Text("1ï¸âƒ£ é»æ“ŠéŸ³è¨ŠéŒ„è£½æŒ‰éˆ•ï¼Œæˆ–ä¸Šå‚³æª”æ¡ˆ")
                    antd.Typography.Text("2ï¸âƒ£ è¼¸å…¥éŸ³è¨Šé€²è¡Œè½‰éŒ„ï¼Œæˆ–èˆ‡æ¨¡å‹é€²è¡Œå¤šæ¨¡æ…‹å°è©±")
                    antd.Typography.Text("3ï¸âƒ£ é»æ“Šæäº¤ä¸¦ç­‰å¾…æ¨¡å‹çš„å›ç­”")
        voice_choice = gr.Dropdown(label="Voice Choice", choices=['Chelsie', 'Ethan'], value='Chelsie', visible=not args.audio_only)
        
        with gr.Tabs():
            with gr.Tab("Online"):
                with gr.Row():
                    with gr.Column(scale=1):
                        microphone = gr.Audio(sources=['microphone'], type="filepath")
                        webcam = gr.Video(sources=['webcam'], height=400, include_audio=True, visible=not args.audio_only)
                        submit_btn = gr.Button("æäº¤", variant="primary")
                        stop_btn = gr.Button("åœæ­¢", visible=False)
                        clear_btn = gr.Button("æ¸…é™¤æ­·å²")
                    with gr.Column(scale=2):
                        media_chatbot = gr.Chatbot(height=650, type="messages")
                def clear_history(): return [], gr.update(value=None), gr.update(value=None)
                def media_predict(audio, video, history, system_prompt, voice_choice): # çŸ­éŸ³è¨Šè™•ç†
                    if audio: history.append({"role": "user", "content": (audio,)})
                    if video: 
                        video_path = video.replace('.webm', '.mp4')
                        ffmpeg.input(video).output(video_path, y='-y').run(quiet=True, overwrite_output=True)
                        history.append({"role": "user", "content": (video_path,)})
                    
                    yield gr.update(value=None), gr.update(value=None), history
                    
                    for updated_history in predict_multimodal(history, system_prompt, voice_choice):
                        yield gr.update(value=None), gr.update(value=None), updated_history

                submit_event = submit_btn.click(fn=media_predict, inputs=[microphone, webcam, media_chatbot, system_prompt_textbox, voice_choice], outputs=[microphone, webcam, media_chatbot])
                stop_btn.click(fn=None, inputs=None, outputs=None, cancels=[submit_event], queue=False)
                clear_btn.click(fn=clear_history, inputs=None, outputs=[media_chatbot, microphone, webcam])


            with gr.Tab("Offline"):
                chatbot = gr.Chatbot(type="messages", height=650)
                with gr.Row(equal_height=True):
                    audio_input = gr.Audio(sources=["upload"], type="filepath", label="ä¸Šå‚³éŸ³è¨Š (é•·/çŸ­)", elem_classes="media-upload", scale=1)
                    image_input = gr.Image(sources=["upload"], type="filepath", label="ä¸Šå‚³åœ–ç‰‡", elem_classes="media-upload", scale=1, visible=not args.audio_only)
                    video_input = gr.Video(sources=["upload"], label="ä¸Šå‚³å½±ç‰‡ (çŸ­)", elem_classes="media-upload", scale=1, visible=not args.audio_only)
                text_input = gr.Textbox(show_label=False, placeholder="è¼¸å…¥æ–‡å­—...")
                with gr.Row():
                    submit_btn_offline = gr.Button("æäº¤", variant="primary", size="lg")
                    stop_btn_offline = gr.Button("åœæ­¢", visible=False, size="lg")
                    clear_btn_offline = gr.Button("æ¸…é™¤æ­·å²", size="lg")
                def clear_chat_history(): return [], gr.update(value=None), gr.update(value=None), gr.update(value=None), gr.update(value=None)
                
                submit_event_offline = gr.on(
                    triggers=[submit_btn_offline.click, text_input.submit], 
                    fn=chat_predict_router, 
                    inputs=[text_input, audio_input, image_input, video_input, chatbot, system_prompt_textbox, voice_choice], 
                    outputs=[text_input, audio_input, image_input, video_input, chatbot]
                )
                stop_btn_offline.click(fn=None, inputs=None, outputs=None, cancels=[submit_event_offline], queue=False)
                clear_btn_offline.click(fn=clear_chat_history, inputs=None, outputs=[chatbot, text_input, audio_input, image_input, video_input])

                gr.HTML("""
                    <style>
                        .media-upload { margin: 10px; min-height: 160px; }
                        .media-upload > .wrap { border: 2px dashed #ccc; border-radius: 8px; padding: 10px; height: 100%; }
                        .media-upload:hover > .wrap { border-color: #666; }
                        .media-upload { flex: 1; min-width: 0; }
                    </style>
                """)
    return demo


def _get_args():
    parser = ArgumentParser()
    parser.add_argument('-c', '--checkpoint-path', type=str, default="Qwen/Qwen2.5-Omni-7B", help='Checkpoint name or path, default to %(default)r')
    parser.add_argument('--cpu-only', action='store_true', help='Run demo with CPU only')
    parser.add_argument('--flash-attn2', action='store_true', default=False, help='Enable flash_attention_2 when loading the model.')
    parser.add_argument('--host', type=str, default='0.0.0.0', help='Demo server name.')
    parser.add_argument('--port', type=int, default=7860, help='Demo server port.')
    parser.add_argument('--audio-only', action='store_true', help='Run in audio-only mode, hiding video components in the UI.')
    parser.add_argument('--segment-duration', type=int, default=60, help='Duration of each audio segment for transcription in seconds.')
    parser.add_argument('--max-new-tokens', type=int, default=1536, help='Maximum new tokens to generate.')
    parser.add_argument('--temperature', type=float, default=0.1, help='Temperature for sampling.')
    parser.add_argument('--repetition-penalty', type=float, default=1.1, help='Repetition penalty.')
    parser.add_argument('--idle-timeout', type=int, default=300, help='Idle timeout in seconds before unloading the model.')
    parser.add_argument('--ui-language', type=str, choices=['en', 'zh'], default='zh', help='Display language for the UI.')
    return parser.parse_args()


if __name__ == "__main__":
    cli_args = _get_args()
    IDLE_TIMEOUT = cli_args.idle_timeout
    gradio_app = _launch_demo(cli_args)
    app = gr.mount_gradio_app(app, gradio_app, path="/")
    
    # å•Ÿå‹•é–’ç½®ç›£æ§ç·šç¨‹
    monitor_thread = threading.Thread(target=idle_monitor, daemon=True)
    monitor_thread.start()
    print(f"[INFO] é–’ç½®ç›£æ§å·²å•Ÿå‹• (è¶…æ™‚æ™‚é–“: {IDLE_TIMEOUT}s)")
    
    print(f"[INFO] å•Ÿå‹•ä¼ºæœå™¨æ–¼ http://{cli_args.host}:{cli_args.port}")
    print(f"[INFO] Gradio UI ä»‹é¢ä½æ–¼ http://{cli_args.host}:{cli_args.port}/")
    print(f"[INFO] Curl API ç«¯é»ä½æ–¼ http://{cli_args.host}:{cli_args.port}/transcribe/")
    import uvicorn
    uvicorn.run(app, host=cli_args.host, port=cli_args.port)

